{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "877ae687-06c4-4115-a17e-9ae23fc8b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b17002e-b037-4035-8a19-199b003116d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0, 1, 2, 3], [1, 0, 3, 0], [0, 4, 2, 3]], {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4})\n"
     ]
    }
   ],
   "source": [
    "def label_to_count(labels):\n",
    "    '''\n",
    "    Given a list of labels, returns a dictionary that maps each class label to how many\n",
    "    instances of that label were present in the list.\n",
    "    '''\n",
    "    label_to_count_dict = {}\n",
    "    for label in labels:\n",
    "        if label not in label_to_count_dict:\n",
    "            label_to_count_dict[label] = 0\n",
    "        label_to_count_dict[label] += 1\n",
    "    return label_to_count_dict\n",
    "\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    '''\n",
    "    Given a list of sequences, will turn into a tokenized vector.\n",
    "    \n",
    "    ARGS:\n",
    "        seqs: a list of strings where every string is a sequence\n",
    "    RETURNS:\n",
    "        tokenized_seqs (list(list(int))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    '''\n",
    "    max_len = 0\n",
    "    \n",
    "    # build up a voc2ind (letters:token)\n",
    "    # based on ATGC and include padding and unknown tokens\n",
    "    voc2ind = {voc:ind for ind,voc in enumerate(['A', 'T', 'C', 'G', 'N'])}\n",
    "    \n",
    "    i = len(voc2ind)\n",
    "    \n",
    "    # tokenize the sequences\n",
    "    tokenized_seqs = []\n",
    "    for seq in seqs:\n",
    "        tokenized_seq = []\n",
    "        for e in seq:\n",
    "            # make sure the sequence is upper case, a == A\n",
    "            seq = seq.upper()\n",
    "            # if we haven't seen this letter before, add to the corupus\n",
    "            if not e in voc2ind:\n",
    "                voc2ind[e] = i\n",
    "                i += 1\n",
    "            tokenized_seq.append(voc2ind[e])\n",
    "        tokenized_seqs.append(tokenized_seq)\n",
    "        \n",
    "    return tokenized_seqs, voc2ind\n",
    "        \n",
    "    \n",
    "def prepare_labels(labels):\n",
    "    '''\n",
    "    Given a list of labels will turn them into integer labels\n",
    "    Args:\n",
    "        labels: a list of labels\n",
    "    Returns:\n",
    "        tokenized_labels: numpy array(list) a list of label tokens\n",
    "        label2token: (dict) a dictionary where keys are letters, values are corresponding token\n",
    "    '''\n",
    "    tokenized_labels = []\n",
    "    label2token = {}\n",
    "    i = 0\n",
    "    for label in labels:\n",
    "        if not label in label2token:\n",
    "            label2token[label] = i\n",
    "            i += 1\n",
    "        tokenized_labels.append(label2token[label])\n",
    "    return tokenized_labels, label2token\n",
    "\n",
    "\n",
    "def pad(tokenized_seqs, voc2ind):\n",
    "    '''\n",
    "    Pad each sequence to the maximum length by adding a <pad> token\n",
    "    \n",
    "    ARGS:\n",
    "        tokenized_seqs (list(list(str))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    RETURNS:\n",
    "        a numpy array of all the tokenized sequences that have been padded to be the same\n",
    "        length.\n",
    "    '''\n",
    "\n",
    "    padded_seqs = []\n",
    "    \n",
    "    # find max sequence length\n",
    "    max_len = 0\n",
    "    for seq in tokenized_seqs:\n",
    "        max_len = max(len(seq), max_len)\n",
    "    \n",
    "    # add padding so sequences are max_length\n",
    "    for seq in tokenized_seqs:\n",
    "        padded_seq = seq + [voc2ind['<pad>']] * (max_len - len(seq))\n",
    "        padded_seqs.append(padded_seq)\n",
    "        \n",
    "    return np.array(padded_seqs, dtype=np.float32)\n",
    "\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"\n",
    "    Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7b1fb1b-589e-499c-baff-a5dca56bc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0, verbose=1):\n",
    "  ''' Trains a neural network. Returns a 2d numpy array, where every list \n",
    "  represents the losses per epoch.\n",
    "  '''\n",
    "  net.to(device)\n",
    "  losses_per_epoch = []\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "  for epoch in range(epochs):\n",
    "    sum_loss = 0.0\n",
    "    losses = []\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses.append(loss.item())\n",
    "        sum_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            if verbose:\n",
    "              print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, sum_loss / 100))\n",
    "            sum_loss = 0.0\n",
    "    # print(len(losses))\n",
    "    losses_per_epoch.append(np.mean(losses))\n",
    "  return losses_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdcc7a9c-63e8-4fbe-b53c-5131e7776f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, dataloader):\n",
    "    '''\n",
    "    Given a trained neural network and a dataloader, computes the accuracy.\n",
    "    Arguments:\n",
    "        net: a neural network\n",
    "        dataloader: a dataloader\n",
    "    Returns:\n",
    "        fraction of examples classified correctly (float)\n",
    "        number of correct examples (int)\n",
    "        number of total examples (float)\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = net(input)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct/total, correct, total\n",
    "\n",
    "def print_eval(net, train_dataloader, test_dataloader):\n",
    "    '''\n",
    "    Given a test and train data loader, prints the test and train accuracy and\n",
    "    the number of examples they got right.\n",
    "    RETURNS\n",
    "        (train_acc, test_acc) results of running accuracy on the two dataloaders\n",
    "    '''\n",
    "    train_acc = accuracy(net, train_dataloader)\n",
    "    test_acc = accuracy(net, test_dataloader)\n",
    "    \n",
    "\n",
    "    print(\"Train accuracy: \" + str(train_acc[0]) + \"\\t(\" + str(train_acc[1]) + \"/\" + str(train_acc[2]) + \")\")\n",
    "    print(\"Test accuracy: \" + str(test_acc[0]) + \"\\t(\" + str(test_acc[1]) + \"/\" + str(test_acc[2]) + \")\")\n",
    "          \n",
    "    return train_acc, test_acc\n",
    "\n",
    "def plot_losses(losses, smooth_val = None, title = \"\"):\n",
    "    '''\n",
    "    Plots the losses per epoch returned by the training function.\n",
    "    Args:\n",
    "        losses: a list of losses returned by train\n",
    "        smooth_val: an optinal integer value if smoothing is desired\n",
    "        title: a title for the graph\n",
    "    '''\n",
    "    # loss = np.mean(losses, axis = 1)\n",
    "    epochs = [i for i in range(1, len(losses) + 1)]\n",
    "    if smooth_val is not None:\n",
    "        lossses = smooth(losses, smooth_val)\n",
    "    plt.plot(epochs, losses, marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    \n",
    "\n",
    "def smooth(x, size):\n",
    "    '''\n",
    "    Given an array, smooths it by some number size, to make it look less janky.\n",
    "    '''\n",
    "    return np.convolve(x, np.ones(size)/size, mode='same')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2e9c622-4d38-4801-8318-42f5d43439f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GGGCAGGAAATCATTCTGGCCCTTCTGCGTGTTTAAGCTTACGTGCGGTTGCTTTTAGGAGGTTCAGTGATTTTTAACAAAATCAAGTTTAGTCTCAGCTCCAAGGCTCCATCCCCTGCACTGTGCTCACCTGTGCAAATTTTACAGTTGAGATCGAAGAGGTGTGCGCGGTGCTGACTGGTGGTGTCTTTCAACATGCTGCTGAAGACGTCGAGAAGCGGCGCTGTGCTCTTCTCAGGGACAGCACGTGCTGACTCTTGCTGTTCCTAAAAAAGAAAAAGAAAAAAAAGTGAGGTCGTTTCTTCTCCAAACTCAGCTCCAAATTAACCACACACAAGAAAAGCAGTCTCATGGGATTGAGACCCACGGGGGAGAAAAAAGGACCATCTAATACAAGGACTGAGGAATGCCACAATGACATACACCTGCTGTAAGCTCAGGTCCTGCCCAATAATTTAAGATAACCTCAAAACATTTGGGATGGATTAGTCTATTCTCGGTGAACACAAAATCTCCCAAATGCTTACCTCTGAATCCGACACTGGCGGAGAGTCCTCCAGATCGGGGATGGCCTCCTGCCTGGGGGCCGTCTTCTTGCTTTCATTGTGCAGTTTAGTTCTGGACTCCATCACCTGAAATGAAAAAGACGAAACAGAGCTTAGGCCTTGTTTTCTAGGAGGAAAGCATCGCTTATGCAGCCGTCTATGAATTTATTTCTATTAAGCTCGAGTCCTGGGTGCCTCCGCAGGTACCCCTCAGCACTCACAGATCTCGCTGGCCTCTCTTTCCACGTGGAAAGC', 'AATGGACTCATAGTTCCACATGGCTGGGGAGGTCTCACAATCATGGCGGAAGGCAAAGGAAGAGCAAAGGCATGTCTTACATGACGACAGGCAAAAGAGCATGTGCAGGGGAACTGTCCTTTATAAAACCATCAGATCTCATGAGACTTATTCACTATCATGAGAACAGCACAGGAAAAACCCACCTCCATGATTCAGTTACCTCCCACTGGGTCCCTCCCATTACTCATGGGGATTATGGGATCTACAATTCAAAATGAGATTTGGGTAGAGACACAGCCAAACCATATCGATGTATGTCCTTAAAAGAGTGAGGCAGGGAGAGATTTCACACAGAAAGAAGAGGAGGAGGAGGCAATGTGTCCACAGAGGCAGAGATTGGAGTGGTGCAGGCTCAGGTCAAGGAATGGTGACAGCTACTAGGAAGTGAATGAAGCAAGGAACTGATTCCCCCAAAGGAGTGTGGCCCTGCTGACATGTTGATTTCAGATTTCTGGCCTCCAGAACTGTGAAAGAATACATTTCTATTGTTTTAAGCCACTCAATTTTTGGTAAGTCAATATAGCCGCTGTAAAAAACTAATACGCTAACAACGAATTCTGTTTCCTCCATTTATTCATACCTTCCCTAATGTCTGCTTTTCCAGGTGAAGTATCTGAAAGGTCTTTTTCCCTTCCATGCTCTTGCCCATCCTTCTGGAATATTCTCTGTGGCAGTTATGCAGACCTTTACAACCACATTTCTTGTGGGTTGTATGTGTGAAATCTTCAGAGCTCAGAAAAAAAAAATCTGATCAAAAG', 'TCTCCAGAGGAAGTAACCTTTGTGTGGAAATCTTTGAATTTGAAAGATTCAAATATGAAATTCAAATATGAAAGACGAATTAAGAGTTAACTCTATGTGGCAGATTGCATTTTCCAAAAATTATGGCAGCCACATACTCTTCCAGAACTTTGCTACTCCGCATTAGAGGAGGAGTCTATTTCTGCTCCCCTGGACTTTGTAACTGCTTTGATGAATAGAATGTAGCAGGGACAACACTGTGTGACTTCTGTGGCTGGGTAATGAAAGTTGGTATGGCTTCTACCTAGCTCTTTCTTGGGACACACCCTTGGGACACAGCCACCAAGCTATTACTAGCCTATGTGGAAATGTTGGGGAAGCAGGAGCCTAGGAGAGCCAGACTAATGCTACTTTAAGTTCAACTCTGTCTTGGAACTAACAAGGCACATTCCTTGCTGGTTACGACCGACAGTTATAAGATATTTACAACGGAAGAAAAGCCTAAAGATTCCTACAAAGACACATTTCTATAACAGTGGAAAGTCAGGATGTCCCAATACCCATAAAAATATATGTTTTCAAGATCATTATATTCTTTGATGTACTCAAACTCTAGAATGTCAAGGATAGTTTTCTTTAAATCAATAGAATAATAAATTTTGTCACGCTGTCTGCTCACATGAGTGTAGACACAGCTTAGCTTAGCTTTTACATGGACAAGACACCTTTATAAGAAAAACTTAAAGAAGAGGCTTTCCTCTGCTTTCTTTCTGAGGACACCCCACCCTGTAAGTGAGTAACTTTCAATAAATTATCTCTTC', 'GACTCCCCAAGAGTGCAACAAGTACCCTGCATCCCCGCATCCCCTGAGTGACGTGCTAGCAGAGGTAATGAGGAAACACCTGTTTCCAACCAGCAGCAACAATGAGTGAAGCAAGCTCCAGTTAAAAGTACACTGTTCTGGAGCTCATGCTAACTGTGGGGTCTTCAGAATGTACATATGTACTTCCTCAGTCTACGCCCACACTGTAGGATCCAGACTTGATTCCAGGTGCTGGGTGGGACAGCTCTCCTGCCAGTGTAACACCCAGATGGTGATTAGTACAAGATCAGGGCAGCCCAGGTGGGAGGCAAACCAGCATAATCCTGCAAACAAAGACTCAATCTGAGGATAAAAATAGATTTTTTTAAAAACCAACACCACTGCATACACACACGTCTTGTGTTAGAAACTTGACTTTCCTAATAGACACTGTGCATAGGTGCTCAGTTAACTATCGGAGAACAAACCAGCTAAAGCCTGGACACGTGCGTCAGCTGACAGTACCCGGAACACGGGGAAGCTTCCATTCTGGGGAATTTCCACTTCTCGAGGATCCCATCAATTTCCTCCTCTCCAAATATTTAGTAAAGAATTTGTGCCTTTCATTTCCAAAGAAAGAGGCTTTCTCCTTATCCCACGTCTGGGTTTATAATGCTCCCTGCTCCCCAAACTTCCTGAAGTTAGAATCATTCCCAGAGTAGCACAGCAGCACCGTGTGAGGAGTGCTGAGTCAGAAAACAAATCTTGTGGGCCTCTCATCGTAAATTGTGAAACGTTTCTGGAAAGCAATGCAGCACTGT', 'ATACAAAATTAGCTGGGCATGGTGGTGCATGCCTGTACTCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATCATTTGAACCTGGGAGGCAGAGGTTGCAGTGAGCCAAGATCATGCCATTGCACTCCAGCCTGGAACAAGACACCAAGAGCAAAACTCCATTTCAAAAAGAAAGAAAAGAAAAGAAAATGAAGAAGTAAAGTCATCTCTATTTGCAGATGACATGACCTTGTACATAGAAAATCCTAAGCCACTGAAAAAATATTAGAACTAGCACTTTGGGAGGCTGAGGTGGGAGGATTGCTTGAGCCCAGGAGTTCAAAACTATCCTGGGCAATGTAGCAAGACCCCAGCTCTACAAAAAAATTAAAAATTAGCCAGGCATGGTGGCACATGCCTATAGTCCCAGCTACTTGGGAGGCTGAGACAGGAGAATCACTTGAGCCTAGGAGCTTAAGGTTGCAGTGAGCTATGATCACACCACTGCACTCCAGCCTGGGTAACAGACCCTGTCCCAAAATAAACAAATAAATAATAACTATTAGAGCTAATAAATGAGTACAACAGGGCTGCAGGGTATAAGATCAATGTATGAAAATCAATTGTGTTTCTATACAGCAGTAATCAACAAACCAAAAGTGAGATGAAGAGAACAATTCCATAATACTTTGGAGTCAACTTAACAAGATAAGTGCAAAACACATATTCTGAAGACGACAATACGTTGTTGAAAGAAATTTTAAAAGACCTAAAGAAGTGGAAAGACATCCTATGTTCATGGGTTGGAAGCCGAATATTTTT']\n",
      "[1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "seqs = []\n",
    "labels = []\n",
    "\n",
    "# every sequence comes as 3 lines. The first line is the name,\n",
    "# the second is the gene\n",
    "# the third is the class\n",
    "\n",
    "f = open('dataset.txt')\n",
    "for i, line in enumerate(f):\n",
    "    if i % 3 == 1:\n",
    "        line = re.sub('[^ATCGatcg]', '', line)\n",
    "        seqs.append(line)\n",
    "    if i%3 == 2:\n",
    "        labels.append(int(line[0]))\n",
    "f.close()\n",
    "\n",
    "print(seqs[:5])\n",
    "print(labels[:5])\n",
    "\n",
    "# tokenizing and getting a vocab\n",
    "tokenized_seqs, voc2ind = prepare_data(seqs)\n",
    "\n",
    "# padding\n",
    "# tokenized_seqs = pad(tokenized_seqs, voc2ind)\n",
    "\n",
    "# tokenizing labels\n",
    "tokenized_labels, label2token = prepare_labels(labels)\n",
    "\n",
    "# Showing the result of this:\n",
    "# print(\"\\n\", tokenized_seqs, \"\\n\\n\", voc2ind, \"\\n\\n\", label_to_count(labels))\n",
    "\n",
    "# Train Test Split\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_seqs, tokenized_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fee27d9c-f6e1-4cf4-9997-8c4896b52a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3643ab1-23c9-41c6-ab28-8abe8b178a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/m3n_w10n31q06s6_0fk1_j800000gn/T/ipykernel_1910/1399588528.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(torch.int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here are some experiments run with the model\n",
    "# kind of represents a table, each row is an experiment.\n",
    "# the first row is the indecies. Each row should be the same length.\n",
    "ex = [['in_channels_conv1', 'out_channels_conv1', 'in_channels_conv2', 'out_channels_conv2', 'in_channels_fc1', \n",
    "       'kernel_size_conv1', 'stride_conv1', 'kenrnel_size_conv2', 'kernel_size_conv1', \n",
    "       'momentum', 'learning rate', 'decay', ('train_acc'), ('test_acc'),],\n",
    "      [800, 480, 480, 200, 4200, 24, 3, 12, 3, 0.8, 0.01, 0.02, (0.8193495693495694), (0.7989311957247829)], #1\n",
    "]\n",
    "i = 1\n",
    "\n",
    "FEATURE_SIZE = ex[i][0]\n",
    "\n",
    "class H3_DSC_by_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_size, num_classes=2):\n",
    "\n",
    "\n",
    "        super(H3_DSC_by_CNN, self).__init__()\n",
    "        # UM?\n",
    "        self.vocab_size = vocab_size\n",
    "        # embeded_dim: Dimension of word vectors.\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=ex[i][0],\n",
    "                               out_channels=ex[i][1], \n",
    "                               kernel_size=ex[i][5],\n",
    "                               stride = ex[i][6],\n",
    "                               padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=ex[i][2],\n",
    "                               out_channels=ex[i][3], \n",
    "                               kernel_size=ex[i][7],\n",
    "                               stride = ex[i][8],\n",
    "                               padding = 1)\n",
    "\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool1d(3, 2, 1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = nn.Linear(ex[i][4], 100)\n",
    "\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=0.6)\n",
    "\n",
    "        # softmax\n",
    "        self.softm = nn.Softmax()\n",
    "\n",
    "\n",
    "    # seen in the network pytorch tutorial\n",
    "    # I've been using this function for years, no idea what it does.\n",
    "    # Would it kill the pytorch tutorial people to document this guy?\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The following line is required to send it through the encoder. Not sure why.\n",
    "        # b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "        #x = F.one_hot(x.to(torch.int64)-2,num_classes=4)\n",
    "        # Encode. \n",
    "        # Output shape: (b, max_len, embedded_dim)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # idk flatten it?\n",
    "        # x = torch.flatten(x, 1)\n",
    "\n",
    "        # Permute shuffles dimensions. Do this to satisfy requirments of nn.Conv1d\n",
    "        # Output shape: (b, embedded_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # flattening?\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # Tring to figure out the shape\n",
    "        # print(x.shape)\n",
    "\n",
    "        # fully linear layer + dropout\n",
    "        x = self.drop(x)\n",
    "        # x = self.softm(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Creating net and training\n",
    "net = H3_DSC_by_CNN(len(voc2ind), FEATURE_SIZE)\n",
    "losses = train(net, train_dataloader, epochs = 20, decay = ex[i][-3], lr=ex[i][-4], momentum=ex[i][-5])\n",
    "\n",
    "# Evaluation\n",
    "plot_losses(losses, title = \"Splice Dataset: Convolutional Network Loss\")\n",
    "print_eval(net, train_dataloader, test_dataloader)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd047df-0a9d-486a-bdea-dac77d8dc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'weights_only.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2101cf48-b174-466d-9f9d-3dcc6f888973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0046, -0.0087, -0.0093,  0.0111,  0.0119,  0.0007],\n",
       "         [-0.0037,  0.0151, -0.0145,  0.0002, -0.0097,  0.0115],\n",
       "         [ 0.0127,  0.0133, -0.0065,  0.0122,  0.0035,  0.0182],\n",
       "         ...,\n",
       "         [ 0.0022,  0.0151,  0.0028, -0.0162,  0.0153,  0.0096],\n",
       "         [ 0.0133, -0.0018, -0.0004,  0.0160,  0.0029,  0.0132],\n",
       "         [ 0.0003,  0.0141,  0.0103, -0.0019,  0.0181,  0.0139]],\n",
       "\n",
       "        [[-0.0022,  0.0125,  0.0167,  0.0148,  0.0037,  0.0027],\n",
       "         [ 0.0129, -0.0064,  0.0156, -0.0064, -0.0179, -0.0084],\n",
       "         [-0.0157, -0.0013,  0.0090,  0.0128, -0.0046,  0.0146],\n",
       "         ...,\n",
       "         [-0.0138,  0.0115,  0.0002, -0.0095,  0.0156,  0.0095],\n",
       "         [-0.0170,  0.0165, -0.0078,  0.0109,  0.0021,  0.0182],\n",
       "         [ 0.0120,  0.0047,  0.0072,  0.0052,  0.0118,  0.0074]],\n",
       "\n",
       "        [[ 0.0061, -0.0071,  0.0143,  0.0098, -0.0184,  0.0008],\n",
       "         [-0.0120,  0.0069,  0.0072, -0.0048, -0.0084, -0.0031],\n",
       "         [-0.0109, -0.0110, -0.0086,  0.0175, -0.0088, -0.0025],\n",
       "         ...,\n",
       "         [ 0.0084,  0.0112, -0.0082,  0.0056,  0.0163, -0.0011],\n",
       "         [-0.0107, -0.0118,  0.0168,  0.0159,  0.0168,  0.0127],\n",
       "         [ 0.0115, -0.0086, -0.0102, -0.0115, -0.0056,  0.0013]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0159,  0.0067, -0.0127,  0.0016,  0.0098, -0.0011],\n",
       "         [-0.0012, -0.0080, -0.0165,  0.0160, -0.0070,  0.0147],\n",
       "         [-0.0184, -0.0092, -0.0047, -0.0121,  0.0051, -0.0181],\n",
       "         ...,\n",
       "         [ 0.0125, -0.0078,  0.0074, -0.0105, -0.0147, -0.0045],\n",
       "         [ 0.0179,  0.0050,  0.0072,  0.0029, -0.0088,  0.0030],\n",
       "         [ 0.0005,  0.0153,  0.0061,  0.0159,  0.0019, -0.0100]],\n",
       "\n",
       "        [[ 0.0084, -0.0151,  0.0109,  0.0176,  0.0009, -0.0123],\n",
       "         [-0.0004, -0.0070,  0.0116, -0.0140,  0.0076, -0.0095],\n",
       "         [ 0.0112, -0.0096, -0.0141,  0.0022,  0.0139, -0.0027],\n",
       "         ...,\n",
       "         [ 0.0058,  0.0017,  0.0020, -0.0178,  0.0122,  0.0183],\n",
       "         [-0.0128, -0.0090, -0.0115,  0.0086,  0.0049, -0.0057],\n",
       "         [-0.0129, -0.0110,  0.0148, -0.0181,  0.0089, -0.0142]],\n",
       "\n",
       "        [[-0.0171, -0.0074, -0.0158, -0.0160, -0.0171, -0.0095],\n",
       "         [ 0.0142,  0.0003, -0.0095,  0.0121, -0.0127,  0.0039],\n",
       "         [ 0.0070,  0.0159,  0.0150, -0.0048,  0.0106,  0.0038],\n",
       "         ...,\n",
       "         [-0.0003,  0.0173, -0.0099,  0.0047, -0.0065,  0.0007],\n",
       "         [ 0.0163,  0.0134,  0.0078,  0.0058,  0.0025,  0.0017],\n",
       "         [-0.0134, -0.0168, -0.0046,  0.0166, -0.0168, -0.0113]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['conv2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4e0e0-bc66-41c9-bb9b-b9c7e13f7676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prdm9",
   "language": "python",
   "name": "prdm9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
