{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ed49ac-2753-4e74-8c74-9d5a57bb7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.code.dataloading.feature_engineering import dummy_func, separate_elemets\n",
    "from trainer.code.dataloading import train_test_splits\n",
    "from trainer.code.dataloading.dataloading import create_tf_ds, load_and_create_dictionary\n",
    "from trainer.code.dataloading.tf_dataloaders import  DNA_tf_dl\n",
    "from trainer.code.dataloading.feature_engineering import dummy_func, separate_elemets\n",
    "from trainer.code.dataloading import train_test_splits\n",
    "from trainer.code.dataloading.tf_dataloaders import  DNA_tf_dl\n",
    "\n",
    "from trainer.code.modeling.tf.models import create_multioutput_model\n",
    "from trainer.code.modeling.tf.models import create_multioutput_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e609435b-02c1-43cf-84af-6c28a71a24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.code.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c54bf9-dc3a-4949-b91c-4ba001f65094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import json\n",
    "from keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00301a97-27fb-42d5-b58e-467bcbe6cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Due to MODULEPATH changes, the following have been reloaded:\n",
      "  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module load cuda cudnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c8de4-08bb-4f64-ba34-5c4d1bb7d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_idx = False\n",
    "midpoint = True\n",
    "test2_interval = 10\n",
    "random_seed = 123\n",
    "batch_size = 128\n",
    "use_rev_compl = True\n",
    "\n",
    "\n",
    "ds_root = ''\n",
    "negexamples_config = {(1000,5000):1.0}\n",
    "fold_fn_names = ['partial_chrom_shuffled','partial_chrom_contig','partial_chrom_contig_alternate','whole_genome_shuffled_k_fold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac9a37-0e59-4541-9c85-f59bc3551688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"181fe15119f612e9e270418216720c40b876b43e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422cd9a-4254-4283-bfd4-c02e8f6b618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.02,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3845b9-b37f-41c0-83a0-76015931b7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_chrom = [list(range(1,23))]\n",
    "seq_len = [100]\n",
    "prefix = ['AA1_hospots']\n",
    "n_folds = 3\n",
    "fold_fn_name = [fold_fn_names[0],fold_fn_names[3]]\n",
    "chrom_idx = [True,False]\n",
    "run_nb = 0\n",
    "\n",
    "def run_experiment(chrom, seq_len, prefix, fold_fn_name, chrom_idx, niter=0,name_run='new_run'):\n",
    "\n",
    "    # prefix = prefix[0]\n",
    "    # chrom = all_chrom[0]\n",
    "    # seq_len = seq_len[0]\n",
    "    # fold_fn_name=fold_fn_name[0]\n",
    "    # chrom_idx=True\n",
    "    # name_run = 'new_run'\n",
    "    \n",
    "    wandb.init(project=\"test-chip\",mode='dryrun',name=name_run)\n",
    "    \n",
    "    batch_size = 128\n",
    "    random_seed = 123\n",
    "    n_folds = 3\n",
    "    train_perc = 0.7\n",
    "    epochs = 10\n",
    "    use_rev_compl = True\n",
    "    midpoint = True\n",
    "    \n",
    "    output_name = [f\"{prefix}\"] \n",
    "    fname_prefix = \"\".join(output_name)\n",
    "    \n",
    "    dataset_local_name = f\"{prefix}_seqlen={seq_len}_multnegs.h5\"\n",
    "    ds_path = '/lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/build_dataset/datasets/' + dataset_local_name\n",
    "    \n",
    "    chroms_to_use = chrom\n",
    "    input_keys = ['seq']\n",
    "    if midpoint:\n",
    "        input_keys.append('midpoint')\n",
    "    if chrom_idx:\n",
    "        input_keys.append('chrom_idx')\n",
    "    \n",
    "    dataset_config = get_dataset_config(dataset_full_name=ds_path,\n",
    "                                        chroms_to_use=chroms_to_use)\n",
    "    \n",
    "    train_test_split_config = get_train_test_split_config(strategy=fold_fn_name,\n",
    "                                                          random_seed=random_seed)\n",
    "    \n",
    "    data_io_config = get_data_io_config(input_keys=input_keys,\n",
    "                                        chroms_to_use=chroms_to_use,\n",
    "                                        use_rev_compl=use_rev_compl,\n",
    "                                        output_name=output_name,\n",
    "                                        batch_size=batch_size)\n",
    "    \n",
    "    model_config = get_model_config(output_name=output_name,\n",
    "                                    chrom_idx=chrom_idx, \n",
    "                                    midpoint=midpoint,\n",
    "                                    seq_len=seq_len,\n",
    "                                    data_io_config=data_io_config)\n",
    "    \n",
    "    k_fold_params = get_k_fold_params(n_folds=n_folds,\n",
    "                                      train_perc=train_perc,\n",
    "                                      random_seed=random_seed)\n",
    "    \n",
    "    chroms_list = dataset_config['chroms_to_use']\n",
    "    input_keys = data_io_config['input_keys']\n",
    "    output_keys = data_io_config['output_keys']\n",
    "    index_keys = data_io_config['index_keys']\n",
    "    n_folds = k_fold_params['n_folds']\n",
    "    train_perc = k_fold_params['train_perc']\n",
    "    test_perc = k_fold_params['test_perc']\n",
    "    random_seed = k_fold_params['random_seed']\n",
    "        \n",
    "    data_dict = load_and_create_dictionary(dataset_config, \n",
    "                                           data_io_config, \n",
    "                                           negexamples_config, \n",
    "                                           'train',\n",
    "                                           chroms_to_use=chroms_to_use)               \n",
    "    data_dict, test2_dict = train_test_splits.train_val_split_dicts_by_interval(data_dict,\n",
    "                                                                                input_keys,\n",
    "                                                                                output_keys,\n",
    "                                                                                index_keys,\n",
    "                                                                                test2_interval)\n",
    "    total_examples = 0\n",
    "    pos_examples = 0\n",
    "    neg_examples = 0\n",
    "    for chrom in data_dict.keys():\n",
    "        o = data_dict[chrom][output_name[0]]\n",
    "        l = o.shape\n",
    "        pos_examples += (o == 1).sum()\n",
    "        neg_examples += (o == 0).sum()\n",
    "        total_examples += l[0]\n",
    "    print(f\"There are {total_examples} examples in the dataset\")\n",
    "    #print(f\"Pos examples: {pos_examples}, ratio: {pos_examples/total_examples:.2f}\")\n",
    "    #print(f\"Neg examples: {neg_examples}, ratio: {neg_examples/total_examples:.2f}\")\n",
    "    \n",
    "    \n",
    "    if fold_fn_name == 'partial_chrom_shuffled':\n",
    "        fold_fn = train_test_splits.partial_chrom_shuffled_k_fold\n",
    "    elif fold_fn_name == 'partial_chrom_contig':\n",
    "        fold_fn = train_test_splits.partial_chrom_contig_k_fold\n",
    "    elif fold_fn_name == 'partial_chrom_contig_alternate':\n",
    "        fold_fn = train_test_splits.partial_chrom_contig_alternate_k_fold\n",
    "    elif fold_fn_name == 'whole_genome_shuffled_k_fold':\n",
    "        fold_fn = train_test_splits.whole_genome_shuffled_k_fold\n",
    "    else:\n",
    "        raise ValueError(\"Wrong fold name\")\n",
    "    \n",
    "    folds = fold_fn(data_dict,\n",
    "                    n_folds,\n",
    "                    chroms_list,\n",
    "                    train_perc,\n",
    "                    test_perc,\n",
    "                    input_keys,\n",
    "                    output_keys,\n",
    "                    index_keys,\n",
    "                    random_seed)\n",
    "    \n",
    "    f = h5py.File(dataset_config['dataset_name'], 'r')\n",
    "    metadata = json.loads(f['/'].attrs[\"metadata\"])\n",
    "    f.close()\n",
    "    \n",
    "    rev_com_dict_str = metadata['num_reverse_compliment_dict']\n",
    "    rev_com_dict = {int(k): v for k, v in rev_com_dict_str.items()}\n",
    "    \n",
    "    use_rc = data_io_config['use_rev_compl_as_input']\n",
    "    \n",
    "    if use_rc:\n",
    "        add_rc = rev_com_dict\n",
    "    else:\n",
    "        add_rc = None\n",
    "    \n",
    "    X_transform_dict = data_io_config['X_transform_dict']\n",
    "    y_transform_dict = data_io_config['y_transform_dict']\n",
    "    batch_size = data_io_config['batch_size']\n",
    "    shuffle_train = data_io_config['shuffle_train']\n",
    "    shuffle_val = data_io_config['shuffle_val']\n",
    "    shuffle_test = data_io_config['shuffle_test']\n",
    "    hparams_dict = model_config['hp_dict']\n",
    "    \n",
    "    fold_train_data, fold_val_data = train_test_splits.merge_folds(folds, 0)\n",
    "    \n",
    "    train_ds = DNA_tf_dl(X_dict=fold_train_data['input'],\n",
    "                             X_transform_dict=X_transform_dict,\n",
    "                             y_dict=fold_train_data['output'],\n",
    "                             y_transform_dict=y_transform_dict,\n",
    "                             rev_comp_dict=add_rc,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle_train,\n",
    "                             reshuffle_on_epoch_end=False)\n",
    "    \n",
    "    val_ds = DNA_tf_dl(X_dict=fold_val_data['input'],\n",
    "                           X_transform_dict=X_transform_dict,\n",
    "                           y_dict=fold_val_data['output'],\n",
    "                           y_transform_dict=y_transform_dict,\n",
    "                           rev_comp_dict=add_rc,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle_val,\n",
    "                           reshuffle_on_epoch_end=False)\n",
    "    \n",
    "    test2_ds = DNA_tf_dl(X_dict=test2_dict['input'],\n",
    "                            X_transform_dict=X_transform_dict,\n",
    "                            y_dict=test2_dict['output'],\n",
    "                            y_transform_dict=y_transform_dict,\n",
    "                            rev_comp_dict=add_rc,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            reshuffle_on_epoch_end=False)\n",
    "    \n",
    "    \n",
    "    #earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=77)\n",
    "    \n",
    "    #callbacks = [[earlystop_callback]]\n",
    "    \n",
    "    metrics=['accuracy', \n",
    "                 tf.keras.metrics.AUC(), \n",
    "                 tf.keras.metrics.Precision(), \n",
    "                 precision_m,\n",
    "                 tf.keras.metrics.Recall(),\n",
    "                 recall_m,\n",
    "                 f1_m]\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_multioutput_model(model_config)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=hparams_dict['lr'])\n",
    "    balance_bce = False\n",
    "    \n",
    "    if balance_bce:\n",
    "        train_pos_ratio_dict = {k:v.sum()/v.shape[0] for k, v in output_keys}\n",
    "        \n",
    "        for o in output_keys:\n",
    "            model_config['outputs'][o]['weight'] /= train_pos_ratio_dict[o]\n",
    "            loss_dict = {o:wrapped_partial(wbce,\n",
    "                                 pos_class_wgt=train_pos_ratio_dict[o]) for o in output_keys}\n",
    "    else:\n",
    "        loss_dict = {o:'binary_crossentropy' for o in hparams_dict['outputs_names']}\n",
    "    \n",
    "    model.compile(loss=loss_dict, \n",
    "                  optimizer=opt,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_ds,\n",
    "                        callbacks=[WandbCallback()],\n",
    "                        verbose=1\n",
    "                        ) \n",
    "    pd.DataFrame(history.history).to_csv(f'models/new_exp/chr_all.seqLen{seq_len}.group{prefix}.{fold_fn_name}.chrom_idx_{str(chrom_idx)}.iter_{niter}.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94063f23-28b5-4688-85ea-13fdcd96e0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 14:30:09.229105: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-11 14:30:09.241143: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-11 14:30:09.241173: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nl10902.narval.calcul.quebec): /proc/driver/nvidia/version does not exist\n",
      "2022-11-11 14:30:09.371177: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143000-2cjyzc68/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143000-2cjyzc68/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143000-2cjyzc68/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143000-2cjyzc68/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143000-2cjyzc68/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143855-2uq4wgvv/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143855-2uq4wgvv/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143855-2uq4wgvv/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143855-2uq4wgvv/files/model-best/assets\n",
      "INFO:tensorflow:Assets written to: /lustre06/project/6065672/otastet1/projects/prdm9_chipseq/recode/train/wandb/offline-run-20221111_143855-2uq4wgvv/files/model-best/assets\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "all_chrom = [list(range(1,23))]\n",
    "seq_len = [3500]\n",
    "prefix = ['AA1_hotspots']\n",
    "n_folds = 3\n",
    "fold_fn_name = [fold_fn_names[3]]\n",
    "chrom_idx = [True,False]\n",
    "run_nb = 0\n",
    "\n",
    "for chrom_use in all_chrom:\n",
    "    for sl in seq_len:\n",
    "        for pr in prefix:\n",
    "            for fn in fold_fn_name:\n",
    "                for use_chrom in chrom_idx:\n",
    "                    for niter in [1]:\n",
    "                        run_nb+=1\n",
    "                        run_experiment(chrom_use, sl, pr, fn, use_chrom, niter, '.'.join(['all_chroms.','sl'+str(sl),pr,fn,'chr_idx_'+str(use_chrom)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d61b7e0-b227-4a1f-b3d2-3cf236713725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 3s 46ms/step - loss: 0.3120 - accuracy: 0.8673 - auc: 0.9398 - precision: 0.8882 - precision_m: 0.7189 - recall: 0.8408 - recall_m: 0.7264 - f1_m: 0.6962\n"
     ]
    }
   ],
   "source": [
    "test2_res = model.evaluate(test2_ds,\n",
    "                           batch_size=batch_size,\n",
    "                           return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed0779-8a6d-454d-bfd5-d6a80966d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
